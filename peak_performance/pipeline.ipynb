{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peak_performance import pipeline as pl\n",
    "from peak_performance import models\n",
    "from peak_performance import plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Specify the absolute path to the raw data files.\n",
    "path = r\"\"\n",
    "\n",
    "# obtain a list of raw data file names\n",
    "raw_data_files = pl.detect_npy(path)\n",
    "print(raw_data_files)\n",
    "\n",
    "# necessary information from the user\n",
    "double_peak = []                # 2: List with Booleans in the same order as raw_data_files. Set to True for a given signal, if the signal contains a double peak, and set to False, if it contains a single peak. Visually check this beforehand.\n",
    "pre_filtering = True            # 3: Set this variable to True if you want to check for peaks before fitting/sampling to potentially save a lot of computation time. If you choose True, then you have to provide an expected retention time for each signal.\n",
    "retention_time_estimate = []    # 4: in case you set pre_filtering to True, give a retention time estimate (float) for each signal in raw_data_files. In case of a double peak, give two retention times (in chronological order) as a tuple containing two floats.\n",
    "peak_width_estimate = 1         # 5: in case you set pre_filtering to True, give a rough estimate of the average peak width in minutes you would expect for your LC-MS method.\n",
    "minimum_sn = 5                  # 6: in case you set pre_filtering to True, give a minimum signal to noise ratio for a signal to be defined as a peak during pre-filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data structure and DataFrame(s) for results \n",
    "df_summary, path = pl.initiate(path)\n",
    "for file in raw_data_files:\n",
    "    # parse the data and extract information from the (standardized) file name\n",
    "    timeseries, acquisition, experiment, precursor_mz, product_mz_start, product_mz_end = pl.parse_data(file)\n",
    "    # instantiate the UserInput class all given information\n",
    "    ui = pl.UserInput(path, raw_data_files, double_peak, retention_time_estimate, peak_width_estimate, pre_filtering, minimum_sn, timeseries, acquisition, experiment, precursor_mz, product_mz_start, product_mz_end)\n",
    "    # calculate initial guesses for pre-filtering and defining prior probability distributions\n",
    "    slope_guess, intercept_guess, noise_guess = models.initial_guesses(ui.timeseries[0], ui.timeseries[1])\n",
    "    # apply pre-sampling filter (if selected)\n",
    "    if pre_filtering:\n",
    "        prefilter = pl.prefiltering(file, ui, noise_guess)\n",
    "        if not prefilter:\n",
    "            # if no peak candidates were found, continue with the next signal\n",
    "            continue\n",
    "    # model selection\n",
    "    if ui.double_peak:\n",
    "        pmodel = models.define_model_doublepeak(ui.timeseries[0], ui.timeseries[1])\n",
    "    else:\n",
    "        pmodel = models.define_model_skew(ui.timeseries[0], ui.timeseries[1])\n",
    "    # sample the chosen model\n",
    "    idata = pl.sampling(pmodel)\n",
    "    # apply post-sampling filter\n",
    "    resample, discard = pl.postfiltering(idata, ui)\n",
    "    # if peak was discarded, continue with the next signal\n",
    "    if discard:\n",
    "        df_summary = pl.report_add_nan_to_summary(ui, df_summary)\n",
    "        continue\n",
    "    # if convergence was not yet reached, sample again with more tuning samples\n",
    "    if resample:\n",
    "        idata = pl.sampling(pmodel, tune = 4000)\n",
    "        resample, discard = pl.postfiltering(idata, ui)\n",
    "        if discard:\n",
    "            continue\n",
    "        if resample:\n",
    "            # if signal was flagged for re-sampling a second time, discard it\n",
    "            # TODO: should this really be disvarded or should the contents of idata be added with an additional comment? (would need to add a comment column)\n",
    "            df_summary = pl.report_add_nan_to_summary(ui, df_summary)\n",
    "            continue\n",
    "    # add inference data to df_summary and save it as an Excel file\n",
    "    df_summary = pl.report_add_data_to_summary(idata, df_summary, ui)\n",
    "    # perform posterior predictive sampling\n",
    "    idata = pl.posterior_predictive_sampling(pmodel, idata)\n",
    "    # save the inference data object in a zip file\n",
    "    pl.report_save_idata(path, idata)\n",
    "# save condesed Excel file with area data\n",
    "pl.report_area_sheet(path, df_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm530",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
