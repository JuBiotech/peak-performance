{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Build a pipeline using Peak Performance's convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "from pathlib import Path\n",
    "from peak_performance import pipeline as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, store the path to a folder containing only the raw data you want to analyze in the `path_raw_data` variable.  \n",
    "You can use a string with a preceding `r` so that the backslashes are recognized correctly or the `Path` method from the `pathlib` package for an OS-independent alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the absolute path to the raw data files (as a str or a Path object), e.g. to the provided example files\n",
    "\n",
    "# path_raw_data = r\"C:\\Users\\niesser\\Desktop\\Local GitLab Repositories\\peak-performance\\example\"\n",
    "path_raw_data = Path(r\"C:\\Users\\niesser\\Desktop\\Local GitLab Repositories\\peak-performance\\example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the specified path as an argument for the `detect_raw_data()` function which returns a list of all files of the given data type in the given path.  \n",
    "If you don't specify a data type, the default is `.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1t1R1Part2_110_109.9_110.1.npy', 'A1t1R1Part2_111_109.9_110.1.npy', 'A1t1R1Part2_111_110.9_111.1.npy', 'A1t1R1Part2_112_110.9_111.1.npy', 'A1t1R1Part2_112_111.9_112.1.npy', 'A2t2R1Part1_132_85.9_86.1.npy', 'A4t4R1Part2_137_72.9_73.1.npy']\n"
     ]
    }
   ],
   "source": [
    "# obtain a list of all raw data file names (including their data type)\n",
    "raw_data_files = pl.detect_raw_data(path_raw_data)\n",
    "print(raw_data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the list of files, provide the following information which is necessary for the pipeline:  \n",
    "1. A dictionary `double_peak` containing the file names (including data type) as keys and Boolean values depending on whether the file contains a __single peak (False)__ or __double peak (True)__.  \n",
    "    \n",
    "2. A Boolean `pre_filtering`. Choose whether you want to filter out obvious false positive signals before sampling to save computation time (__True__) or not (__False__). If \"False\" was chosen, skip 3 - 5.\n",
    "    \n",
    "    3. If `pre_filtering` was set to __True__: Provide a dictionary `retention_time_estimate` containing the file names (including data type) as keys and a rough retention time estimate of the compound pertaining to each given raw data file.  \n",
    "  \n",
    "    4. If `pre_filtering` was set to __True__: A float or integer `peak_width_estimate` containing a rough estimate of the average peak width of your LC-MS/MS method (in minutes).  \n",
    "  \n",
    "    5. If `pre_filtering` was set to __True__: A float or integer `minimum_sn` defining a lower threshold of the signal-to-noise ratio which has to be exceed for a signal to be accepted as a peak during pre-filtering. \n",
    "  \n",
    "6) A Boolean `plotting`. Choose whether to create plots (__True__) or not (__False__). In the latter case, the pipeline will only yield the Excel data report sheet with all results and inference data objects for each sampled signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the previously acquired list raw_data_files, this is the easiest way to define the double_peak dictionary\n",
    "# the list with the Booleans needs to be in the same order as raw_data_files\n",
    "double_peak = dict(zip(raw_data_files, 5*[False] + [True] + [False]))       \n",
    "\n",
    "pre_filtering = True                \n",
    "retention_time_estimate = dict(zip(raw_data_files, 5*[26.2] + [(11.7, 12.5)] + [26.2]))    \n",
    "peak_width_estimate = 1             \n",
    "minimum_sn = 5                      \n",
    "\n",
    "plotting = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When triggering the pipeline, a folder for the results named after the current data and time will be created automatically in the directory with the raw data files.  \n",
    "The path to this folder will be returned and stored in the `results` variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pl.pipeline(path_raw_data = path_raw_data,\n",
    "    raw_data_file_format = \".npy\",\n",
    "    pre_filtering = pre_filtering,\n",
    "    double_peak = double_peak,\n",
    "    retention_time_estimate = retention_time_estimate,\n",
    "    peak_width_estimate = peak_width_estimate,\n",
    "    minimum_sn = minimum_sn,\n",
    "    plotting = plotting,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the __inference data objects__ for all signals were saved in the path stored in `results`, you can open any one you are interested in with the command `idata = az.from_netcdf()`.  \n",
    "These objects contain not only the timeseries of the particular signal but also samples from the prior predictive, posterior, and posterior predictive sampling.  \n",
    "This allows you to explore the data in detail and/or build your own plots different from the ones featured in Peak Performance.  \n",
    "  \n",
    "It is highly recommended to check the documentations for `pymc` and `arviz` to get information and inspiration for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an inference data object\n",
    "idata = az.from_netcdf(results / \"A1t1R1Part2_1_110_109.9_110.1\")\n",
    "idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the summary in the DataFrame az_summary\n",
    "az_summary = az.summary(idata)\n",
    "az_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm530",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
